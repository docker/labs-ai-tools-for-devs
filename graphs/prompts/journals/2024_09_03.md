- Run some prompts checked in to GitHub against a project in the current working directory.
  id:: 66d779c7-c1b7-40c6-a635-fa712da492de
  ```sh
  docker run 
    --rm -it \
    --pull=always \
    -v /var/run/docker.sock:/var/run/docker.sock \
    -v /run/host-services/backend.sock:/host-services/docker-desktop-backend.sock \
    -e "DOCKER_DESKTOP_SOCKET_PATH=/host-services/docker-desktop-backend.sock" \
    --mount type=volume,source=docker-prompts,target=/prompts \
    --mount type=bind,source=$HOME/.openai-api-key,target=/root/.openai-api-key \
    vonwig/prompts:latest \
      run \
      --host-dir $PWD \
      --platform $(uname -o) \
      --prompts "github:docker/labs-make-runbook?ref=main&path=prompts/lazy_docker"
  ```
	- Most of this is boiler plate except:
		- the `--user` option in line 10 requires a valid DOCKER_HUB user name
		- the `--prompts` option in line 12 requires a valid [github reference]([[GitHub Refs]]) to some markdown prompts
		- if the project is located somewhere other than $PWD then the `--host-dir` will need to be updated.
- Run a local prompt markdown file against a project in the current working directory.  In this example, the prompts are not pulled from GitHub.  Instead, our prompts are being developed in a directory called `$PROMPTS_DIR`.  In this example, the local prompt file is `$PROMPTS_DIR/myprompts.md`.
  id:: 66d77f1b-1684-480d-ad7b-5e9f53292fe4
  ```sh
  docker run 
    --rm -it \
    --pull=always \
    -v /var/run/docker.sock:/var/run/docker.sock \
    -v /run/host-services/backend.sock:/host-services/docker-desktop-backend.sock \
    -e "DOCKER_DESKTOP_SOCKET_PATH=/host-services/docker-desktop-backend.sock" \
    --mount type=volume,source=docker-prompts,target=/prompts \
    --mount type=bind,source=$HOME/.openai-api-key,target=/root/.openai-api-key \
    --mount type=bind,source=$PROMPTS_DIR,target=/app/workdir \
    --workdir /app/workdir \
    vonwig/prompts:latest \
      run \
      --host-dir $PWD \
      --platform $(uname -o) \
      --prompts-file myprompts.md
  ```
	- Most of this is boiler plate except:
		- the `--user` option in line 12 requires a valid DOCKER_HUB user name
		- the `--prompts-file` option in line 14 is a relative path to a prompts file (relative to $PROMPTS_DIR)
		- if the project being analyzed is located somewhere other than $PWD then the `--host-dir` will need to be updated.
- [[Running the Prompt Engine]]
- [[Authoring Prompts]]
- Here is a prompt file with lots of non-default metadata (it uses  [extractors]([[Prompt Extractors]]), a [[tool]], and uses a local llm in [[ollama]]).  It uses one system prompt, and one user prompt.  Note that the user prompt contains a moustache template to pull data in from an extractor.
  id:: 66d7f3ff-8769-40b3-b6b5-fc4fceea879e
  
  ```md
  ---
  extractors:
    - name: linguist
      image: vonwig/go-linguist:latest
      command:
      - -json
      output-handler: linguist
  tools:
    - name: findutils-by-name
      description: find files in a project by name
      parameters:
          type: object
          properties:
            glob:
              type: string
              description: the glob pattern for files that should be found
      container:
          image: vonwig/findutils:latest
          command:
            - find
            - .
            - -name
  model: llama3.1
  url:  http://localhost/v1/chat/completions
  stream: false
  ---
  
  # Prompt system
  
  You are an expert on analyzing project content.
  
  # Prompt user
  
  {{#linguist}}
  This project contains {{language}} code.
  {{/linguist}}
  
  Can you find any language specific project files and list them?
  
  ```